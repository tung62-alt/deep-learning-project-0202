# -*- coding: utf-8 -*-
"""pipeline_align_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FxtCb6iqghZAXDLES4FmHWQCfUd0owHd
"""

# -*- coding: utf-8 -*-
"""
pipeline_align_train.py

功能：
1) 讀取 train.csv + (可選) Sentences_Oare_FirstWord_LinNum.csv
2) 對齊檢查：
   - row-level shift 探測（用符號↔英文關鍵詞 plausibility）
   - 句首錨點命中率（針對 train.oare_id 能對到 sentences.text_uuid 的子集）
3) 依閾值標記/過濾可疑樣本（避免錯配污染訓練）
4) ByT5 訓練 + dev 評估（Kaggle: sqrt(BLEU * chrF++)）
5) (可選) 對 test 做推論輸出 submission.csv

相容：
- Colab：可先在 cell 內 pip install
- 本機：請先 pip install transformers datasets accelerate sacrebleu sentencepiece evaluate torch

注意：
- Sentences_Oare_FirstWord_LinNum.csv 的 translation 不保證與 train.csv 同一套翻譯，
  因此我們只把它當「句子錨點（first word/line number）」來評估 transliteration 是否合理對齊，
  不拿它的 translation 當訓練 target。
"""

import os
import re
import json
import math
import random
import argparse
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd

import torch
from torch.utils.data import Dataset

from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    set_seed,
)

import sacrebleu


# -----------------------------
# 0) Utils: reproducibility
# -----------------------------
def seed_everything(seed: int = 42):
    set_seed(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


# -----------------------------
# 1) Cleaning / Normalization
#    - X: transliteration (aggressive + placeholders)
#    - Y: translation (conservative)
# -----------------------------
CONTROL_CHARS = re.compile(r"[\u0000-\u001F\u007F-\u009F\u200B\u200C\u200D\uFEFF]")


def normalize_unicode_nfkc(text: str) -> str:
    import unicodedata
    return unicodedata.normalize("NFKC", text)


def remove_control_chars(text: str) -> str:
    return CONTROL_CHARS.sub("", text)


def normalize_whitespace(text: str) -> str:
    text = re.sub(r"\s+", " ", text)
    return text.strip()


SUBSCRIPT_MAP = str.maketrans("₀₁₂₃₄₅₆₇₈₉", "0123456789")


def normalize_subscripts(text: str) -> str:
    return text.translate(SUBSCRIPT_MAP)


def normalize_gaps(text: str) -> str:
    text = re.sub(r"\[x\]", "<gap>", text)
    text = re.sub(r"\[\.\.\.\s*\.\.\.\]", "<big_gap>", text)
    text = re.sub(r"\.{3,}", "<big_gap>", text)
    return text


def strip_angle_brackets_keep_text(text: str) -> str:
    return re.sub(r"<([^>]+)>", r"\1", text)


def strip_square_brackets_keep_text(text: str) -> str:
    return re.sub(r"\[([^\]]+)\]", r"\1", text)


DET_MAP_SHORT = {
    "d": "DG",
    "mul": "ST",
    "ki": "KI",
    "lu₂": "LU",
    "e₂": "E2",
    "uru": "UR",
    "kur": "KR",
    "mi": "MI",
    "m": "M",
    "geš": "GS",
    "ĝeš": "GS",
    "tug₂": "TG",
    "dub": "DB",
    "id₂": "ID",
    "mušen": "MS",
    "na₄": "NA4",
    "kuš": "KS",
    "u₂": "U2",
}
DET_PATTERN = re.compile(r"\{([^}]+)\}")


def determinatives_to_placeholders(text: str) -> str:
    def repl(m):
        det = m.group(1).strip()
        tag = DET_MAP_SHORT.get(det, det)
        return f"<{tag}>"
    return DET_PATTERN.sub(repl, text)


def space_around_placeholders(text: str) -> str:
    text = re.sub(r"(<[^>]+>)", r" \1 ", text)
    return text


def clean_transliteration_x(text: str) -> str:
    if pd.isna(text):
        return ""
    text = str(text)
    text = normalize_unicode_nfkc(text)
    text = remove_control_chars(text)

    text = normalize_gaps(text)

    text = determinatives_to_placeholders(text)
    text = space_around_placeholders(text)

    text = strip_angle_brackets_keep_text(text)
    text = strip_square_brackets_keep_text(text)

    text = re.sub(r"[⌜⌝]", "", text)
    text = re.sub(r"[!?]", "", text)

    text = text.replace("/", " ")
    text = re.sub(r"[:.]", " ", text)

    text = normalize_subscripts(text)
    text = normalize_whitespace(text)
    return text


def clean_translation_y(text: str) -> str:
    if pd.isna(text):
        return ""
    text = str(text)
    text = normalize_unicode_nfkc(text)
    text = remove_control_chars(text)

    text = strip_angle_brackets_keep_text(text)

    text = normalize_whitespace(text)
    return text


def is_trivially_bad_text(s: str) -> bool:
    if s is None:
        return True
    s = str(s).strip()
    if len(s) < 3:
        return True
    # only dots / ellipsis
    if re.fullmatch(r"[.\s…]+", s):
        return True
    return False


# -----------------------------
# 2) Alignment diagnostics
#    2.1 row shift detection (plausibility)
#    2.2 anchors hit-rate (from Sentences file)
# -----------------------------
LEX_PLAUSIBILITY = {
    "KIŠIB": ["seal"],
    "DUMU": ["son"],
    "TÚG": ["textile", "cloth"],
    "GÍN": ["shekel", "silver"],
    "KÙ.BABBAR": ["silver"],
    "IŠTAR": ["ishtar", "ištar"],
    "URU": ["city"],
}


def plausibility_score(translit: str, trans: str) -> float:
    tl = "" if pd.isna(translit) else str(translit)
    tr = "" if pd.isna(trans) else str(trans).lower()
    total = 0
    hit = 0
    for k, vs in LEX_PLAUSIBILITY.items():
        if k in tl:
            total += 1
            if any(v in tr for v in vs):
                hit += 1
    if total == 0:
        return float("nan")  # no anchors -> ignore
    return hit / total


def detect_best_shift(train_df: pd.DataFrame, max_shift: int = 3) -> Tuple[int, Dict[int, float]]:
    scores_by_shift = {}
    for s in range(-max_shift, max_shift + 1):
        shifted = train_df["translation"].shift(s)
        scores = []
        for a, b in zip(train_df["transliteration"], shifted):
            sc = plausibility_score(a, b)
            if not np.isnan(sc):
                scores.append(sc)
        scores_by_shift[s] = float(np.mean(scores)) if scores else float("nan")

    # best = max mean score among non-nan
    best_shift = 0
    best_val = -1.0
    for s, v in scores_by_shift.items():
        if np.isnan(v):
            continue
        if v > best_val:
            best_val = v
            best_shift = s
    return best_shift, scores_by_shift


def load_sentence_anchors(sent_path: str) -> Optional[pd.DataFrame]:
    if not sent_path or not os.path.exists(sent_path):
        return None
    sent = pd.read_csv(sent_path)
    needed = ["text_uuid", "sentence_obj_in_text", "line_number", "first_word_spelling", "first_word_transcription"]
    for c in needed:
        if c not in sent.columns:
            raise ValueError(f"Sentences file missing column: {c}")
    anchors = sent[needed].copy()
    anchors = anchors.sort_values(["text_uuid", "sentence_obj_in_text"]).reset_index(drop=True)
    return anchors


def anchors_hit_rate_for_row(x_clean: str, anchors_for_text: pd.DataFrame) -> float:
    """
    在同一個 text_uuid 的 anchors 中，逐一嘗試在 x_clean 內按順序找到句首詞；
    命中率越高表示 transliteration 內容更像是「同一份 text」的合理排列。
    """
    if pd.isna(x_clean):
        return float("nan")
    text = str(x_clean)
    if len(text) < 5:
        return 0.0

    cursor = 0
    hits = 0
    total = 0

    for _, r in anchors_for_text.iterrows():
        w1 = "" if pd.isna(r["first_word_spelling"]) else str(r["first_word_spelling"]).strip()
        w2 = "" if pd.isna(r["first_word_transcription"]) else str(r["first_word_transcription"]).strip()
        # 沒句首詞就跳過
        if not w1 and not w2:
            continue
        total += 1

        idx = -1
        if w1:
            idx = text.find(w1, cursor)
        if idx == -1 and w2:
            idx = text.find(w2, cursor)

        if idx != -1:
            hits += 1
            cursor = idx + max(len(w1), len(w2), 1)

    if total == 0:
        return float("nan")
    return hits / total


def add_alignment_flags(
    train_df: pd.DataFrame,
    anchors: Optional[pd.DataFrame],
    suspicious_plausibility_th: float = 0.0,
    suspicious_anchor_th: float = 0.15,
) -> pd.DataFrame:
    """
    產出欄位：
    - plausibility: 規則關鍵詞粗對齊分數（越高越好）
    - anchor_hit_rate: 句首錨點命中率（僅對 oare_id 能 match text_uuid 的列；否則 NaN）
    - suspicious: 是否可疑（任一條件觸發）
    """
    df = train_df.copy()

    df["plausibility"] = [
        plausibility_score(a, b) for a, b in zip(df["X_clean"], df["Y_clean"])
    ]

    df["anchor_hit_rate"] = np.nan
    if anchors is not None and "oare_id" in df.columns:
        anchor_map = {}
        # groupby 會比較快
        for text_uuid, g in anchors.groupby("text_uuid", sort=False):
            anchor_map[text_uuid] = g
        hit_rates = []
        for oid, x in zip(df["oare_id"], df["X_clean"]):
            if oid in anchor_map:
                hit_rates.append(anchors_hit_rate_for_row(x, anchor_map[oid]))
            else:
                hit_rates.append(np.nan)
        df["anchor_hit_rate"] = hit_rates

    def is_suspicious(row) -> bool:
        # 明顯壞文本
        if is_trivially_bad_text(row["X_clean"]) or is_trivially_bad_text(row["Y_clean"]):
            return True

        # plausibility 太低（有錨點時才有效；nan 不判定）
        p = row["plausibility"]
        if not np.isnan(p) and p <= suspicious_plausibility_th:
            return True

        # anchors 命中率太低（只對有值者判定；nan 不判定）
        ahr = row["anchor_hit_rate"]
        if not np.isnan(ahr) and ahr < suspicious_anchor_th:
            return True

        return False

    df["suspicious"] = df.apply(is_suspicious, axis=1)
    return df


# -----------------------------
# 3) Dev split (by X length buckets)
# -----------------------------
def stratified_dev_split(df: pd.DataFrame, dev_ratio: float, seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:
    rng = np.random.default_rng(seed)
    lengths = df["X_clean"].str.len().values
    q1, q2 = np.quantile(lengths, [0.33, 0.66])
    bucket = np.where(lengths <= q1, 0, np.where(lengths <= q2, 1, 2))
    tmp = df.copy()
    tmp["_bucket"] = bucket

    dev_idx = []
    for b in [0, 1, 2]:
        idx = tmp.index[tmp["_bucket"] == b].tolist()
        rng.shuffle(idx)
        take = int(round(len(idx) * dev_ratio))
        dev_idx.extend(idx[:take])

    dev = tmp.loc[dev_idx].drop(columns=["_bucket"]).reset_index(drop=True)
    trn = tmp.drop(index=dev_idx).drop(columns=["_bucket"]).reset_index(drop=True)
    return trn, dev


# -----------------------------
# 4) Metrics: Kaggle style sqrt(BLEU * chrF++)
# -----------------------------
def compute_kaggle_score(preds: List[str], refs: List[str]) -> Dict[str, float]:
    bleu = sacrebleu.corpus_bleu(preds, [refs]).score / 100.0
    chrf = sacrebleu.corpus_chrf(preds, [refs], word_order=2).score / 100.0
    score = math.sqrt(max(bleu, 1e-12) * max(chrf, 1e-12))
    return {"bleu": bleu, "chrfpp": chrf, "kaggle_score": score}


def byt5_safe_decode_batch(seqs, tokenizer):
    outs = []
    for seq in seqs:
        seq = [int(t) for t in seq if int(t) != -100]
        seq = [t for t in seq if t not in tokenizer.all_special_ids]
        tokens = tokenizer.convert_ids_to_tokens(seq)
        text = tokenizer.convert_tokens_to_string(tokens)
        text = re.sub(r"\s+", " ", text).strip()
        outs.append(text)
    return outs


# -----------------------------
# 5) Train / Inference
# -----------------------------
def train_model(
    model_name: str,
    tokenizer,
    train_df: pd.DataFrame,
    dev_df: pd.DataFrame,
    out_dir: str,
    max_source_len: int,
    max_target_len: int,
    seed: int,
    lr: float = 3e-4,
    epochs: int = 10,
    per_device_bs: int = 4,
    grad_accum: int = 4,
    num_beams: int = 4,
    fp16: bool = False,
):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    model.config.pad_token_id = tokenizer.pad_token_id

    def tokenize_df(df):
        enc = tokenizer(
            df["X_clean"].tolist(),
            max_length=max_source_len,
            truncation=True,
            padding=True,
        )
        lab = tokenizer(
            text_target=df["Y_clean"].tolist(),
            max_length=max_target_len,
            truncation=True,
            padding=True,
        )
        enc["labels"] = lab["input_ids"]
        return enc

    train_enc = tokenize_df(train_df)
    dev_enc = tokenize_df(dev_df)

    class EncDataset(torch.utils.data.Dataset):
        def __init__(self, encodings):
            self.enc = encodings
        def __len__(self):
            return len(self.enc["input_ids"])
        def __getitem__(self, idx):
            return {k: torch.tensor(v[idx]) for k, v in self.enc.items()}

    train_ds = EncDataset(train_enc)
    dev_ds = EncDataset(dev_enc)

    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

    args = Seq2SeqTrainingArguments(
        output_dir=out_dir,
        learning_rate=lr,
        per_device_train_batch_size=per_device_bs,
        per_device_eval_batch_size=per_device_bs,
        gradient_accumulation_steps=grad_accum,
        num_train_epochs=epochs,

        eval_strategy="no",
        save_strategy="no",
        logging_steps=50,

        predict_with_generate=False,
        generation_num_beams=num_beams,
        generation_max_length=max_target_len,

        load_best_model_at_end=True,
        metric_for_best_model="eval_kaggle_score",
        greater_is_better=True,

        save_total_limit=2,
        fp16=fp16 and torch.cuda.is_available(),
        report_to="none",
        seed=seed,
    )

    def compute_metrics(eval_pred):
        preds_ids, labels_ids = eval_pred
        labels_ids = np.where(labels_ids == -100, tokenizer.pad_token_id, labels_ids)
        preds = byt5_safe_decode_batch(preds_ids, tokenizer)
        refs = byt5_safe_decode_batch(labels_ids, tokenizer)
        s = compute_kaggle_score(preds, refs)
        return {
            "eval_bleu": s["bleu"],
            "eval_chrfpp": s["chrfpp"],
            "eval_kaggle_score": s["kaggle_score"],
        }

    trainer = Seq2SeqTrainer(
        model=model,
        args=args,
        train_dataset=train_ds,
        eval_dataset=dev_ds,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    trainer.train()
    trainer.save_model(out_dir)
    tokenizer.save_pretrained(out_dir)
    return out_dir


@torch.no_grad()
def generate_predictions(
    model_dir: str,
    tokenizer,
    x_list: List[str],
    max_source_len: int,
    max_new_tokens: int,
    batch_size: int = 16,
    num_beams: int = 4,
) -> List[str]:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device)
    model.eval()

    preds = []
    for i in range(0, len(x_list), batch_size):
        batch = x_list[i:i+batch_size]
        enc = tokenizer(
            batch,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_source_len
        ).to(device)

        gen = model.generate(
            **enc,
            num_beams=num_beams,
            max_new_tokens=max_new_tokens,
        )
        out = byt5_safe_decode_batch(gen.detach().cpu().numpy(), tokenizer)
        preds.extend(out)

    return preds


# -----------------------------
# 6) Main pipeline
# -----------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--train_path", type=str, required=True, help="train.csv path")
    ap.add_argument("--sentences_path", type=str, default="", help="Sentences_Oare_FirstWord_LinNum.csv path (optional)")
    ap.add_argument("--test_path", type=str, default="", help="test.csv path (optional)")
    ap.add_argument("--sample_sub_path", type=str, default="", help="sample_submission.csv path (optional)")
    ap.add_argument("--out_dir", type=str, required=True)

    ap.add_argument("--model_name", type=str, default="google/byt5-small")
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--dev_ratio", type=float, default=0.1)

    ap.add_argument("--max_source_len", type=int, default=512)
    ap.add_argument("--max_target_len", type=int, default=256)

    # alignment filters
    ap.add_argument("--auto_shift_fix", action="store_true",
                    help="If best shift != 0, apply shift to translation column before cleaning (dangerous if not real shift).")
    ap.add_argument("--filter_suspicious", action="store_true",
                    help="Drop suspicious rows before training.")
    ap.add_argument("--suspicious_plausibility_th", type=float, default=0.0)
    ap.add_argument("--suspicious_anchor_th", type=float, default=0.15)

    # training
    ap.add_argument("--epochs", type=int, default=10)
    ap.add_argument("--lr", type=float, default=3e-4)
    ap.add_argument("--per_device_bs", type=int, default=4)
    ap.add_argument("--grad_accum", type=int, default=4)
    ap.add_argument("--num_beams", type=int, default=4)
    ap.add_argument("--fp16", action="store_true")

    # inference
    ap.add_argument("--do_infer", action="store_true",
                    help="Run inference on test and write submission.csv (requires test_path + sample_sub_path).")

    args = ap.parse_args()
    os.makedirs(args.out_dir, exist_ok=True)
    seed_everything(args.seed)

    # --- Load train ---
    train_df = pd.read_csv(args.train_path)
    if "transliteration" not in train_df.columns or "translation" not in train_df.columns:
        raise ValueError("train.csv must contain columns: transliteration, translation")
    if "oare_id" not in train_df.columns:
        # 若你資料沒有 oare_id，anchors_hit_rate 就無法 join，但其它仍能跑
        train_df["oare_id"] = ""

    # --- Shift diagnostics (raw columns) ---
    best_shift, shift_scores = detect_best_shift(train_df, max_shift=3)
    with open(os.path.join(args.out_dir, "shift_scores.json"), "w", encoding="utf-8") as f:
        json.dump({"best_shift": best_shift, "scores": shift_scores}, f, ensure_ascii=False, indent=2)
    print(f"[Alignment] shift score by s=-3..3: {shift_scores}")
    print(f"[Alignment] best_shift = {best_shift}")

    # Optional: apply shift fix (only if you are confident it's real)
    if args.auto_shift_fix and best_shift != 0:
        # 注意：這是整體平移修正，請只在你確定是 row shift 時啟用
        train_df["translation"] = train_df["translation"].shift(best_shift)
        train_df = train_df.dropna(subset=["translation"]).reset_index(drop=True)
        print(f"[Alignment] Applied translation shift = {best_shift}. New train rows = {len(train_df)}")

    # --- Clean ---
    train_df["X_clean"] = train_df["transliteration"].apply(clean_transliteration_x)
    train_df["Y_clean"] = train_df["translation"].apply(clean_translation_y)

    # Drop obvious empties
    train_df = train_df[(train_df["X_clean"].str.len() > 0) & (train_df["Y_clean"].str.len() > 0)].reset_index(drop=True)

    # --- Anchors (optional) + flags ---
    anchors = load_sentence_anchors(args.sentences_path) if args.sentences_path else None
    flagged = add_alignment_flags(
        train_df,
        anchors=anchors,
        suspicious_plausibility_th=args.suspicious_plausibility_th,
        suspicious_anchor_th=args.suspicious_anchor_th,
    )
    flagged.to_csv(os.path.join(args.out_dir, "train_flagged.csv"), index=False)
    print(f"[Alignment] flagged saved: {os.path.join(args.out_dir, 'train_flagged.csv')}")
    print(f"[Alignment] suspicious rate = {flagged['suspicious'].mean():.3f}")

    # --- Filter suspicious (optional) ---
    if args.filter_suspicious:
        before = len(flagged)
        flagged = flagged[~flagged["suspicious"]].reset_index(drop=True)
        after = len(flagged)
        print(f"[Alignment] filtered suspicious: {before} -> {after}")
    flagged.to_csv(os.path.join(args.out_dir, "train_clean_final.csv"), index=False)

    # --- Dev split ---
    train_set, dev_set = stratified_dev_split(flagged, args.dev_ratio, args.seed)
    train_set.to_csv(os.path.join(args.out_dir, "train_gold.csv"), index=False)
    dev_set.to_csv(os.path.join(args.out_dir, "dev_gold.csv"), index=False)
    print(f"[Split] train={len(train_set)}, dev={len(dev_set)}")

    # --- Train ---
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    model_out = os.path.join(args.out_dir, "student_final")
    os.makedirs(model_out, exist_ok=True)

    model_dir = train_model(
        model_name=args.model_name,
        tokenizer=tokenizer,
        train_df=train_set,
        dev_df=dev_set,
        out_dir=model_out,
        max_source_len=args.max_source_len,
        max_target_len=args.max_target_len,
        seed=args.seed,
        lr=args.lr,
        epochs=args.epochs,
        per_device_bs=args.per_device_bs,
        grad_accum=args.grad_accum,
        num_beams=args.num_beams,
        fp16=args.fp16,
    )
    print(f"[Train] model saved to: {model_dir}")

    # --- Eval on dev (local score) ---
    dev_preds = generate_predictions(
        model_dir=model_dir,
        tokenizer=tokenizer,
        x_list=dev_set["X_clean"].tolist(),
        max_source_len=args.max_source_len,
        max_new_tokens=args.max_target_len,
        batch_size=16,
        num_beams=args.num_beams,
    )
    dev_refs = dev_set["Y_clean"].tolist()
    scores = compute_kaggle_score(dev_preds, dev_refs)
    with open(os.path.join(args.out_dir, "dev_scores.json"), "w", encoding="utf-8") as f:
        json.dump(scores, f, ensure_ascii=False, indent=2)
    print(f"[Eval] DEV scores: {scores}")

    # --- Inference (optional) ---
    if args.do_infer:
        if not args.test_path or not args.sample_sub_path:
            raise ValueError("--do_infer requires --test_path and --sample_sub_path")

        test_df = pd.read_csv(args.test_path)
        sub_df = pd.read_csv(args.sample_sub_path)

        if "transliteration" not in test_df.columns:
            raise ValueError("test.csv must contain column: transliteration")

        test_df["X_clean"] = test_df["transliteration"].apply(clean_transliteration_x)
        test_df = test_df[test_df["X_clean"].str.len() > 0].reset_index(drop=True)

        test_preds = generate_predictions(
            model_dir=model_dir,
            tokenizer=tokenizer,
            x_list=test_df["X_clean"].tolist(),
            max_source_len=args.max_source_len,
            max_new_tokens=args.max_target_len,
            batch_size=16,
            num_beams=args.num_beams,
        )

        # align to sample_submission
        if "id" in test_df.columns and "id" in sub_df.columns:
            pred_df = pd.DataFrame({"id": test_df["id"].values, "translation": test_preds})
            sub = sub_df[["id"]].merge(pred_df, on="id", how="left")
        else:
            sub = sub_df.copy()
            sub["translation"] = test_preds[:len(sub)]

        out_sub = os.path.join(args.out_dir, "submission.csv")
        sub.to_csv(out_sub, index=False)
        print(f"[Infer] submission saved: {out_sub}")


if __name__ == "__main__":
    main()