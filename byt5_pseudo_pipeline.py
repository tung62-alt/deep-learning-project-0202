# -*- coding: utf-8 -*-
"""byt5_pseudo_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-EUxSVzH7UGxxWwUc3ini0qCqSkAtND4
"""

# -*- coding: utf-8 -*-
import os, re, json, math, random, argparse
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset

from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments, Seq2SeqTrainer,
    set_seed
)
import sacrebleu

# -----------------------------
# Basic utils
# -----------------------------
def seed_everything(seed: int):
    set_seed(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def ensure_dir(p): os.makedirs(p, exist_ok=True)

# -----------------------------
# Cleaning (same spirit as你之前那支)
# -----------------------------
CONTROL_CHARS = re.compile(r"[\u0000-\u001F\u007F-\u009F\u200B\u200C\u200D\uFEFF]")
SUBSCRIPT_MAP = str.maketrans("₀₁₂₃₄₅₆₇₈₉", "0123456789")
DET_MAP_SHORT = {"d":"DG","mul":"ST","ki":"KI","lu₂":"LU","e₂":"E2","uru":"UR","kur":"KR","mi":"MI","m":"M","geš":"GS","ĝeš":"GS","tug₂":"TG","dub":"DB","id₂":"ID","mušen":"MS","na₄":"NA4","kuš":"KS","u₂":"U2"}
DET_PATTERN = re.compile(r"\{([^}]+)\}")

def normalize_unicode_nfkc(text: str) -> str:
    import unicodedata
    return unicodedata.normalize("NFKC", text)

def remove_control(text: str) -> str:
    return CONTROL_CHARS.sub("", text)

def normalize_ws(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()

def normalize_gaps(text: str) -> str:
    text = re.sub(r"\[x\]", "<gap>", text)
    text = re.sub(r"\.{3,}", "<big_gap>", text)
    return text

def det_to_placeholder(text: str) -> str:
    def repl(m):
        det = m.group(1).strip()
        tag = DET_MAP_SHORT.get(det, det)
        return f"<{tag}>"
    return DET_PATTERN.sub(repl, text)

def strip_angle_keep(text: str) -> str:
    return re.sub(r"<([^>]+)>", r"\1", text)

def strip_square_keep(text: str) -> str:
    return re.sub(r"\[([^\]]+)\]", r"\1", text)

def clean_x(s: str) -> str:
    if pd.isna(s): return ""
    s = normalize_unicode_nfkc(str(s))
    s = remove_control(s)
    s = normalize_gaps(s)
    s = det_to_placeholder(s)
    s = re.sub(r"(<[^>]+>)", r" \1 ", s)
    s = strip_angle_keep(s)
    s = strip_square_keep(s)
    s = s.translate(SUBSCRIPT_MAP)
    s = s.replace("/", " ")
    s = re.sub(r"[:.]", " ", s)
    s = re.sub(r"[⌜⌝!?]", "", s)
    return normalize_ws(s)

def clean_y(s: str) -> str:
    if pd.isna(s): return ""
    s = normalize_unicode_nfkc(str(s))
    s = remove_control(s)
    s = strip_angle_keep(s)
    return normalize_ws(s)

def is_bad_text(s: str) -> bool:
    if s is None: return True
    s = str(s).strip()
    if len(s) < 3: return True
    if re.fullmatch(r"[.\s…]+", s): return True
    return False

# -----------------------------
# Metric (Kaggle sqrt(BLEU*chrF++))
# -----------------------------
def kaggle_score(preds: List[str], refs: List[str]) -> Dict[str,float]:
    bleu = sacrebleu.corpus_bleu(preds, [refs]).score / 100.0
    chrf = sacrebleu.corpus_chrf(preds, [refs], word_order=2).score / 100.0
    s = math.sqrt(max(bleu, 1e-12) * max(chrf, 1e-12))
    return {"bleu": bleu, "chrfpp": chrf, "kaggle_score": s}

def byt5_safe_decode(ids, tok):
    # ids: (seq_len,) list/int array
    ids = [int(t) for t in ids if int(t) != -100]
    ids = [t for t in ids if t not in tok.all_special_ids]
    tokens = tok.convert_ids_to_tokens(ids)
    text = tok.convert_tokens_to_string(tokens)
    return re.sub(r"\s+", " ", text).strip()

def decode_batch(arr, tok):
    return [byt5_safe_decode(x, tok) for x in arr]

# -----------------------------
# Split
# -----------------------------
def stratified_dev_split(df: pd.DataFrame, dev_ratio: float, seed: int):
    rng = np.random.default_rng(seed)
    L = df["X_clean"].str.len().values
    q1, q2 = np.quantile(L, [0.33, 0.66])
    b = np.where(L<=q1,0,np.where(L<=q2,1,2))
    tmp = df.copy()
    tmp["_b"] = b
    dev_idx = []
    for k in [0,1,2]:
        idx = tmp.index[tmp["_b"]==k].tolist()
        rng.shuffle(idx)
        take = int(round(len(idx)*dev_ratio))
        dev_idx += idx[:take]
    dev = tmp.loc[dev_idx].drop(columns=["_b"]).reset_index(drop=True)
    trn = tmp.drop(index=dev_idx).drop(columns=["_b"]).reset_index(drop=True)
    return trn, dev

# -----------------------------
# Seq2Seq Trainer
# -----------------------------
class EncDataset(Dataset):
    def __init__(self, enc):
        self.enc = enc
    def __len__(self): return len(self.enc["input_ids"])
    def __getitem__(self, i):
        return {k: torch.tensor(v[i]) for k,v in self.enc.items()}

def make_enc(tokenizer, X: List[str], Y: Optional[List[str]], max_src: int, max_tgt: int):
    enc = tokenizer(X, max_length=max_src, truncation=True, padding=True)
    if Y is not None:
        lab = tokenizer(text_target=Y, max_length=max_tgt, truncation=True, padding=True)
        enc["labels"] = lab["input_ids"]
    return enc

def train_byt5(
    model_name_or_dir: str,
    out_dir: str,
    train_df: pd.DataFrame,
    dev_df: pd.DataFrame,
    seed: int,
    max_src: int,
    max_tgt: int,
    lr: float,
    epochs: int,
    bs: int,
    grad_accum: int,
    num_beams_eval: int,
    fp16: bool,
    bf16:bool,
    do_eval_during_train: bool,
):
    ensure_dir(out_dir)
    tok = AutoTokenizer.from_pretrained(model_name_or_dir)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_dir)

    if tok.pad_token_id is None:
        tok.pad_token_id = tok.eos_token_id
    model.config.pad_token_id = tok.pad_token_id

    tr_enc = make_enc(tok, train_df["X_clean"].tolist(), train_df["Y_clean"].tolist(), max_src, max_tgt)
    dv_enc = make_enc(tok, dev_df["X_clean"].tolist(), dev_df["Y_clean"].tolist(), max_src, max_tgt)
    tr_ds = EncDataset(tr_enc)
    dv_ds = EncDataset(dv_enc)

    collator = DataCollatorForSeq2Seq(tokenizer=tok, model=model)

    args = Seq2SeqTrainingArguments(
        output_dir=out_dir,
        learning_rate=lr,
        per_device_train_batch_size=bs,
        per_device_eval_batch_size=bs,
        gradient_accumulation_steps=grad_accum,
        num_train_epochs=epochs,
        logging_steps=50,
        report_to="none",
        seed=seed,
        bf16=bf16 and torch.cuda.is_available(),
        fp16=fp16 and torch.cuda.is_available() and (not bf16),

        # 你之前卡住的主因：每個epoch eval+generate 很慢
        # 這裡做成開關
        eval_strategy="epoch" if do_eval_during_train else "no",
        save_strategy="epoch" if do_eval_during_train else "no",
        predict_with_generate=do_eval_during_train,
        generation_num_beams=num_beams_eval,
        generation_max_length=max_tgt,
        load_best_model_at_end=do_eval_during_train,
        metric_for_best_model="eval_kaggle_score",
        greater_is_better=True,
        save_total_limit=2,
    )

    def compute_metrics(eval_pred):
        preds_ids, labels_ids = eval_pred
        labels_ids = np.where(labels_ids == -100, tok.pad_token_id, labels_ids)
        preds = decode_batch(preds_ids, tok)
        refs  = decode_batch(labels_ids, tok)
        s = kaggle_score(preds, refs)
        return {"eval_bleu": s["bleu"], "eval_chrfpp": s["chrfpp"], "eval_kaggle_score": s["kaggle_score"]}

    trainer = Seq2SeqTrainer(
        model=model,
        args=args,
        train_dataset=tr_ds,
        eval_dataset=dv_ds,
        data_collator=collator,
        compute_metrics=compute_metrics if do_eval_during_train else None,
    )
    trainer.train()
    trainer.save_model(out_dir)
    tok.save_pretrained(out_dir)
    return out_dir

@torch.no_grad()
def generate_with_logprob(
    model_dir: str,
    X: List[str],
    max_src: int,
    max_new_tokens: int,
    batch_size: int,
    num_beams: int,
    fp16: bool,
    bf16: bool = False,
):
    """
    產生 pseudo-label 同時回傳一個粗略置信度：avg token logprob
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    tok = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device)
    model.eval()

    if tok.pad_token_id is None:
        tok.pad_token_id = tok.eos_token_id
    model.config.pad_token_id = tok.pad_token_id

    preds, confs = [], []

    for i in range(0, len(X), batch_size):
        batch = X[i:i+batch_size]
        enc = tok(batch, return_tensors="pt", padding=True, truncation=True, max_length=max_src).to(device)

        gen = model.generate(
            **enc,
            num_beams=num_beams,
            max_new_tokens=max_new_tokens,
            return_dict_in_generate=True,
            output_scores=True,
        )
        seqs = gen.sequences  # (B, T)
        texts = decode_batch(seqs.detach().cpu().numpy(), tok)

        # 估 logprob：用 scores（每步的logits after softmax）
        # scores: list length = generated_steps, each (B, vocab)
        # 取每步選到 token 的 logprob 平均（忽略第一個 decoder token）
        step_scores = gen.scores
        # 取生成 token ids（去掉輸入部分，generate 對 encoder-decoder 通常 sequences 只含 decoder）
        # 這裡簡化：把每一步的 argmax/beam token 取出
        # 對 beam search 的 exact mapping 比較複雜；但作為粗置信度，直接用每步 max logprob 也可
        # 這裡用「每步 max logprob」的平均，保守當置信度 proxy
        lp = []
        for sc in step_scores:
            # sc: (B, vocab) logits
            logp = torch.log_softmax(sc, dim=-1)  # (B, vocab)
            lp.append(logp.max(dim=-1).values.detach().cpu().numpy())  # (B,)
        if len(lp) == 0:
            avg_lp = np.zeros(len(texts), dtype=float)
        else:
            avg_lp = np.mean(np.stack(lp, axis=0), axis=0)  # (B,)

        preds.extend(texts)
        confs.extend(avg_lp.tolist())

    return preds, confs

def eval_dev(model_dir: str, dev_df: pd.DataFrame, max_src: int, max_tgt: int, bs: int, num_beams: int):
    tok = AutoTokenizer.from_pretrained(model_dir)
    preds, _ = generate_with_logprob(
        model_dir=model_dir,
        X=dev_df["X_clean"].tolist(),
        max_src=max_src,
        max_new_tokens=max_tgt,
        batch_size=bs,
        num_beams=num_beams,
        fp16=False,
        bf16=True,
    )
    refs = dev_df["Y_clean"].tolist()
    return kaggle_score(preds, refs)

# -----------------------------
# Pseudo-label mixing
# -----------------------------
def make_mixed_train(
    gold_df: pd.DataFrame,
    pseudo_df: pd.DataFrame,
    pseudo_weight: float,
    seed: int
) -> pd.DataFrame:
    """
    最簡單可控的混合：用「重複抽樣」近似權重
    pseudo_weight=0.3 表示 pseudo 大約是 gold 的 30% 份量
    """
    rng = np.random.default_rng(seed)
    if pseudo_weight <= 0:
        return gold_df.copy().reset_index(drop=True)

    n_gold = len(gold_df)
    n_pseudo = int(round(n_gold * pseudo_weight))
    if n_pseudo <= 0:
        return gold_df.copy().reset_index(drop=True)

    # 從 pseudo_df 抽 n_pseudo 筆（可放回）
    idx = rng.integers(0, len(pseudo_df), size=n_pseudo)
    sampled = pseudo_df.iloc[idx].copy()
    mix = pd.concat([gold_df, sampled], ignore_index=True)
    mix = mix.sample(frac=1.0, random_state=seed).reset_index(drop=True)
    return mix

def filter_pseudo_by_conf(pseudo_df: pd.DataFrame, conf_th: float) -> pd.DataFrame:
    if "conf" not in pseudo_df.columns:
        return pseudo_df
    return pseudo_df[pseudo_df["conf"] >= conf_th].reset_index(drop=True)

# -----------------------------
# Main stages
# -----------------------------
def main():
    ap = argparse.ArgumentParser()

    # paths
    ap.add_argument("--train_csv", required=True)
    ap.add_argument("--flagged_csv", required=True, help="train_flagged.csv (含 suspicious 欄)")
    ap.add_argument("--test_csv", required=True)
    ap.add_argument("--sample_sub_csv", required=True)
    ap.add_argument("--unlabeled_csv", default="", help="8000 unlabeled pool (transliteration only)")

    ap.add_argument("--out_dir", required=True)
    ap.add_argument("--seed", type=int, default=42)

    # model
    ap.add_argument("--base_model", default="google/byt5-small")
    ap.add_argument("--max_src", type=int, default=512)
    ap.add_argument("--max_tgt", type=int, default=256)

    # training params
    ap.add_argument("--epochs", type=int, default=10)
    ap.add_argument("--lr", type=float, default=3e-4)
    ap.add_argument("--bs", type=int, default=8)          # A100 可拉高
    ap.add_argument("--grad_accum", type=int, default=2)
    ap.add_argument("--dev_ratio", type=float, default=0.1)
    ap.add_argument("--fp16", action="store_true")
    ap.add_argument("--bf16", action="store_true")

    # generation params
    ap.add_argument("--gen_bs", type=int, default=32)     # A100 可拉高
    ap.add_argument("--gen_beams", type=int, default=1)   # pseudo 建議 1（快且穩）
    ap.add_argument("--eval_beams", type=int, default=2)  # dev eval 可 2

    # pseudo control
    ap.add_argument("--conf_th", type=float, default=-2, help="pseudo confidence threshold (avg max-logprob)")
    ap.add_argument("--pseudo_weight", type=float, default=0.5, help="pseudo amount vs gold in mixing")

    # unlabeled control (8000 pool)
    ap.add_argument("--u_part", choices=["A","B"], default="A")
    ap.add_argument("--u_take", type=int, default=2000, help="how many from part to label")
    ap.add_argument("--u_seed", type=int, default=123)

    # stage switches
    ap.add_argument("--do_repair", action="store_true")
    ap.add_argument("--do_selfdistill", action="store_true")
    ap.add_argument("--do_label_unlabeled_byt5", action="store_true")
    ap.add_argument("--do_final_train", action="store_true")
    ap.add_argument("--do_infer", action="store_true")

    args = ap.parse_args()
    ensure_dir(args.out_dir)
    seed_everything(args.seed)

    # -----------------------
    # Load & clean gold
    # -----------------------
    train_raw = pd.read_csv(args.train_csv)
    flagged = pd.read_csv(args.flagged_csv)

    # 做一致清理
    flagged["X_clean"] = flagged["transliteration"].apply(clean_x)
    flagged["Y_clean"] = flagged["translation"].apply(clean_y)

    # 保留非空
    flagged = flagged[(flagged["X_clean"].str.len() > 0) & (flagged["Y_clean"].str.len() > 0)].reset_index(drop=True)

    # 固定 dev split（用你目前 filtered 後 1468 的那套比較合理）
    gold_filtered = flagged[~flagged["suspicious"]].reset_index(drop=True)
    train_gold, dev_gold = stratified_dev_split(gold_filtered, args.dev_ratio, args.seed)

    save_meta = {
        "n_all": len(flagged),
        "n_filtered": len(gold_filtered),
        "n_train_gold": len(train_gold),
        "n_dev_gold": len(dev_gold),
    }
    with open(os.path.join(args.out_dir, "meta.json"), "w", encoding="utf-8") as f:
        json.dump(save_meta, f, ensure_ascii=False, indent=2)

    # -----------------------
    # Stage 1: train base teacher
    # -----------------------
    teacher0_dir = os.path.join(args.out_dir, "teacher0_byt5")
    if not os.path.exists(os.path.join(teacher0_dir, "config.json")):
        train_byt5(
            model_name_or_dir=args.base_model,
            out_dir=teacher0_dir,
            train_df=train_gold,
            dev_df=dev_gold,
            seed=args.seed,
            max_src=args.max_src,
            max_tgt=args.max_tgt,
            lr=args.lr,
            epochs=args.epochs,
            bs=args.bs,
            grad_accum=args.grad_accum,
            num_beams_eval=args.eval_beams,
            fp16=args.fp16,
            bf16=args.bf16,
            do_eval_during_train=False,  # 關掉訓練中 eval，避免你之前那種每epoch卡很久
        )

    base_scores = eval_dev(teacher0_dir, dev_gold, args.max_src, args.max_tgt, bs=args.gen_bs, num_beams=args.eval_beams)
    print("[Teacher0] DEV:", base_scores)
    with open(os.path.join(args.out_dir, "teacher0_dev.json"), "w", encoding="utf-8") as f:
        json.dump(base_scores, f, ensure_ascii=False, indent=2)

    # -----------------------
    # Stage 2: Repair suspicious (用 teacher0 重標 suspicious)
    # -----------------------
    repaired_csv = os.path.join(args.out_dir, "gold_repaired.csv")
    if args.do_repair:
        sus = flagged[flagged["suspicious"]].reset_index(drop=True)
        keep = flagged[~flagged["suspicious"]].reset_index(drop=True)

        if len(sus) > 0:
            preds, confs = generate_with_logprob(
                model_dir=teacher0_dir,
                X=sus["X_clean"].tolist(),
                max_src=args.max_src,
                max_new_tokens=args.max_tgt,
                batch_size=args.gen_bs,
                num_beams=args.gen_beams,
                fp16=args.fp16,
                bf16=args.bf16,
            )
            sus2 = sus.copy()
            sus2["Y_pseudo"] = preds
            sus2["conf"] = confs
            sus2 = sus2[~sus2["Y_pseudo"].apply(is_bad_text)].reset_index(drop=True)
            sus2 = filter_pseudo_by_conf(sus2, args.conf_th)

            # 用 pseudo 修補 suspicious 的 target
            sus2["Y_clean"] = sus2["Y_pseudo"]
            sus2 = sus2.drop(columns=["Y_pseudo"])

            repaired = pd.concat([keep, sus2], ignore_index=True)
        else:
            repaired = flagged.copy()

        repaired = repaired.sample(frac=1.0, random_state=args.seed).reset_index(drop=True)
        repaired.to_csv(repaired_csv, index=False)
        print("[Repair] saved:", repaired_csv)
    else:
        if os.path.exists(repaired_csv):
            repaired = pd.read_csv(repaired_csv)
        else:
            repaired = flagged.copy()

    # 重新產生「修補後的 filtered gold」：疑似錯配已被修補，所以可以全納入 gold
    repaired["X_clean"] = repaired["transliteration"].apply(clean_x)
    repaired["Y_clean"] = repaired["translation"].apply(clean_y)
    repaired = repaired[(repaired["X_clean"].str.len() > 0) & (repaired["Y_clean"].str.len() > 0)].reset_index(drop=True)

    train_gold2, dev_gold2 = stratified_dev_split(repaired, args.dev_ratio, args.seed)

    # -----------------------
    # Stage 3: Self-distillation on repaired gold (對 1468/或整個 repaired 產 pseudo 再混合)
    # -----------------------
    student1_dir = os.path.join(args.out_dir, "student1_selfdistill")
    pseudo_all_csv = os.path.join(args.out_dir, "pseudo_all_repaired.csv")

    if args.do_selfdistill:
        # 先對「repaired gold 的 train split」產 pseudo，避免把 dev 泄漏進訓練
        preds, confs = generate_with_logprob(
            model_dir=teacher0_dir,
            X=train_gold2["X_clean"].tolist(),
            max_src=args.max_src,
            max_new_tokens=args.max_tgt,
            batch_size=args.gen_bs,
            num_beams=args.gen_beams,
            fp16=args.fp16,
            bf16=args.bf16,
        )
        pseudo_df = train_gold2.copy()
        pseudo_df["Y_clean"] = preds
        pseudo_df["conf"] = confs
        pseudo_df = pseudo_df[~pseudo_df["Y_clean"].apply(is_bad_text)].reset_index(drop=True)
        pseudo_df = filter_pseudo_by_conf(pseudo_df, args.conf_th)
        pseudo_df.to_csv(pseudo_all_csv, index=False)
        print("[SelfDistill] pseudo saved:", pseudo_all_csv, "n=", len(pseudo_df))

        # 混合：gold (repaired) + pseudo(低權重)
        mixed = make_mixed_train(train_gold2, pseudo_df, pseudo_weight=args.pseudo_weight, seed=args.seed)
        mixed.to_csv(os.path.join(args.out_dir, "mixed_train_student1.csv"), index=False)

        train_byt5(
            model_name_or_dir=teacher0_dir,   # 用 teacher0 當初始化更穩
            out_dir=student1_dir,
            train_df=mixed,
            dev_df=dev_gold2,
            seed=args.seed,
            max_src=args.max_src,
            max_tgt=args.max_tgt,
            lr=args.lr * 0.6,                 # self-distill 建議稍降 lr
            epochs=max(3, args.epochs//2),     # 不用跑滿10，通常 3~5 足夠
            bs=args.bs,
            grad_accum=args.grad_accum,
            num_beams_eval=args.eval_beams,
            fp16=args.fp16,
            bf16=args.bf16,
            do_eval_during_train=False,
        )

        s1 = eval_dev(student1_dir, dev_gold2, args.max_src, args.max_tgt, bs=args.gen_bs, num_beams=args.eval_beams)
        print("[Student1] DEV:", s1)
        with open(os.path.join(args.out_dir, "student1_dev.json"), "w", encoding="utf-8") as f:
            json.dump(s1, f, ensure_ascii=False, indent=2)
    else:
        if not os.path.exists(os.path.join(student1_dir, "config.json")):
            student1_dir = teacher0_dir

    # -----------------------
    # Stage 4: ByT5 label unlabeled pool (8000 的一部分)
    # -----------------------
    unlabeled_pseudo_csv = os.path.join(args.out_dir, f"unlabeled_{args.u_part}_byt5_pseudo.csv")
    if args.do_label_unlabeled_byt5:
        if not args.unlabeled_csv:
            raise ValueError("--unlabeled_csv required for do_label_unlabeled_byt5")

        U = pd.read_csv(args.unlabeled_csv)
        # 期待至少有 transliteration 欄
        if "transliteration" not in U.columns:
            raise ValueError("unlabeled_csv must have column: transliteration")

        U["X_clean"] = U["transliteration"].apply(clean_x)
        U = U[U["X_clean"].str.len() > 0].reset_index(drop=True)

        # split A/B
        rng = np.random.default_rng(args.u_seed)
        idx = np.arange(len(U))
        rng.shuffle(idx)
        mid = len(U)//2
        if args.u_part == "A":
            pick = idx[:mid]
        else:
            pick = idx[mid:]
        pick = pick[:min(args.u_take, len(pick))]
        U_part = U.iloc[pick].copy().reset_index(drop=True)

        # 用 student1（若有）產 pseudo（比 teacher0 更強通常更好）
        teacher_for_u = student1_dir if os.path.exists(os.path.join(student1_dir, "config.json")) else teacher0_dir
        preds, confs = generate_with_logprob(
            model_dir=teacher_for_u,
            X=U_part["X_clean"].tolist(),
            max_src=args.max_src,
            max_new_tokens=args.max_tgt,
            batch_size=args.gen_bs,
            num_beams=args.gen_beams,
            fp16=args.fp16,
            bf16=args.bf16,
        )
        U_part["Y_clean"] = preds
        U_part["conf"] = confs
        U_part = U_part[~U_part["Y_clean"].apply(is_bad_text)].reset_index(drop=True)
        U_part = filter_pseudo_by_conf(U_part, args.conf_th)

        U_part.to_csv(unlabeled_pseudo_csv, index=False)
        print("[Unlabeled ByT5 pseudo] saved:", unlabeled_pseudo_csv, "n=", len(U_part))

    # -----------------------
    # Stage 5: Final train ByT5 on gold + (unlabeled pseudo) + (optionally) more pseudo
    # -----------------------
    final_dir = os.path.join(args.out_dir, "byt5_final")
    if args.do_final_train:
        # gold：用 repaired 全量（更乾淨）
        gold_all = repaired.copy()
        gold_all["X_clean"] = gold_all["transliteration"].apply(clean_x)
        gold_all["Y_clean"] = gold_all["translation"].apply(clean_y)
        gold_all = gold_all[(gold_all["X_clean"].str.len()>0) & (gold_all["Y_clean"].str.len()>0)].reset_index(drop=True)

        # unlabeled pseudo（如果有）
        pseudo_list = []
        if os.path.exists(unlabeled_pseudo_csv):
            pseudo_list.append(pd.read_csv(unlabeled_pseudo_csv))
        # 你如果之後還有 LLM pseudo，也可以再 append 進來（同欄位 X_clean, Y_clean 即可）
        # pseudo_list.append(pd.read_csv("unlabeled_B_llm_pseudo.csv"))

        if len(pseudo_list) > 0:
            pseudo_u = pd.concat(pseudo_list, ignore_index=True)
            pseudo_u = pseudo_u[["X_clean","Y_clean"]].dropna().reset_index(drop=True)
            mixed_final = make_mixed_train(gold_all, pseudo_u, pseudo_weight=args.pseudo_weight, seed=args.seed)
        else:
            mixed_final = gold_all

        train_final, dev_final = stratified_dev_split(mixed_final, args.dev_ratio, args.seed)

        # curriculum：先混合訓練，再用 gold 微調一小段（可選，這裡直接做兩段）
        stage1_dir = os.path.join(args.out_dir, "byt5_final_stage1")
        train_byt5(
            model_name_or_dir=student1_dir,    # 用 student1 初始化
            out_dir=stage1_dir,
            train_df=train_final,
            dev_df=dev_final,
            seed=args.seed,
            max_src=args.max_src,
            max_tgt=args.max_tgt,
            lr=args.lr,
            epochs=max(3, args.epochs//2),
            bs=args.bs,
            grad_accum=args.grad_accum,
            num_beams_eval=args.eval_beams,
            fp16=args.fp16,
            bf16=args.bf16,
            do_eval_during_train=False,
        )

        # stage2：只用 gold_all 微調（把模型拉回真標籤分佈）
        train_gold3, dev_gold3 = stratified_dev_split(gold_all, args.dev_ratio, args.seed)
        train_byt5(
            model_name_or_dir=stage1_dir,
            out_dir=final_dir,
            train_df=train_gold3,
            dev_df=dev_gold3,
            seed=args.seed,
            max_src=args.max_src,
            max_tgt=args.max_tgt,
            lr=args.lr * 0.5,
            epochs=2,
            bs=args.bs,
            grad_accum=args.grad_accum,
            num_beams_eval=args.eval_beams,
            fp16=args.fp16,
            bf16=args.bf16,
            do_eval_during_train=False,
        )

        fscore = eval_dev(final_dir, dev_gold3, args.max_src, args.max_tgt, bs=args.gen_bs, num_beams=args.eval_beams)
        print("[Final ByT5] DEV:", fscore)
        with open(os.path.join(args.out_dir, "byt5_final_dev.json"), "w", encoding="utf-8") as f:
            json.dump(fscore, f, ensure_ascii=False, indent=2)

    # -----------------------
    # Stage 6: Inference to submission
    # -----------------------
    if args.do_infer:
        model_for_test = final_dir if os.path.exists(os.path.join(final_dir, "config.json")) else (student1_dir if os.path.exists(os.path.join(student1_dir,"config.json")) else teacher0_dir)

        test = pd.read_csv(args.test_csv)
        sub = pd.read_csv(args.sample_sub_csv)

        if "transliteration" not in test.columns:
            raise ValueError("test.csv must have transliteration column")

        test["X_clean"] = test["transliteration"].apply(clean_x)
        preds, _ = generate_with_logprob(
            model_dir=model_for_test,
            X=test["X_clean"].tolist(),
            max_src=args.max_src,
            max_new_tokens=args.max_tgt,
            batch_size=args.gen_bs,
            num_beams=max(2, args.eval_beams),   # test 可稍微提高 beam
            fp16=args.fp16,
            bf16=args.bf16,
        )

        # 對齊 sample_submission
        if "id" in sub.columns and "id" in test.columns:
            pred_df = pd.DataFrame({"id": test["id"].values, "translation": preds})
            out = sub[["id"]].merge(pred_df, on="id", how="left")
        else:
            out = sub.copy()
            out["translation"] = preds[:len(out)]

        out_path = os.path.join(args.out_dir, "submission.csv")
        out.to_csv(out_path, index=False)
        print("[Infer] saved:", out_path)

if __name__ == "__main__":
    main()