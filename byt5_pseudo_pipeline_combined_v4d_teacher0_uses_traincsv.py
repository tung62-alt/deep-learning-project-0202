# -*- coding: utf-8 -*-
"""byt5_pseudo_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-EUxSVzH7UGxxWwUc3ini0qCqSkAtND4
"""

# -*- coding: utf-8 -*-
import os, re, json, math, random, argparse
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset

from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments, Seq2SeqTrainer,
    set_seed
)
import sacrebleu

# -----------------------------
# Basic utils
# -----------------------------
def seed_everything(seed: int):
    set_seed(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def ensure_dir(p): os.makedirs(p, exist_ok=True)

# -----------------------------
# Cleaning (same spirit as你之前那支)
# -----------------------------
CONTROL_CHARS = re.compile(r"[\u0000-\u001F\u007F-\u009F\u200B\u200C\u200D\uFEFF]")
SUBSCRIPT_MAP = str.maketrans("₀₁₂₃₄₅₆₇₈₉", "0123456789")
DET_MAP_SHORT = {"d":"DG","mul":"ST","ki":"KI","lu₂":"LU","e₂":"E2","uru":"UR","kur":"KR","mi":"MI","m":"M","geš":"GS","ĝeš":"GS","tug₂":"TG","dub":"DB","id₂":"ID","mušen":"MS","na₄":"NA4","kuš":"KS","u₂":"U2"}
DET_PATTERN = re.compile(r"\{([^}]+)\}")

def normalize_unicode_nfkc(text: str) -> str:
    import unicodedata
    return unicodedata.normalize("NFKC", text)

def remove_control(text: str) -> str:
    return CONTROL_CHARS.sub("", text)

def normalize_ws(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()

def normalize_gaps(text: str) -> str:
    text = re.sub(r"\[x\]", "<gap>", text)
    text = re.sub(r"\.{3,}", "<big_gap>", text)
    return text

def det_to_placeholder(text: str) -> str:
    def repl(m):
        det = m.group(1).strip()
        tag = DET_MAP_SHORT.get(det, det)
        return f"<{tag}>"
    return DET_PATTERN.sub(repl, text)

def strip_angle_keep(text: str) -> str:
    return re.sub(r"<([^>]+)>", r"\1", text)

def strip_square_keep(text: str) -> str:
    return re.sub(r"\[([^\]]+)\]", r"\1", text)

def clean_x(s: str) -> str:
    if pd.isna(s): return ""
    s = normalize_unicode_nfkc(str(s))
    s = remove_control(s)
    s = normalize_gaps(s)
    s = det_to_placeholder(s)
    s = re.sub(r"(<[^>]+>)", r" \1 ", s)
    s = strip_angle_keep(s)
    s = strip_square_keep(s)
    s = s.translate(SUBSCRIPT_MAP)
    s = s.replace("/", " ")
    s = re.sub(r"[:.]", " ", s)
    s = re.sub(r"[⌜⌝!?]", "", s)
    return normalize_ws(s)

def clean_y(s: str) -> str:
    if pd.isna(s): return ""
    s = normalize_unicode_nfkc(str(s))
    s = remove_control(s)
    s = strip_angle_keep(s)
    return normalize_ws(s)

def is_bad_text(s: str) -> bool:
    if s is None: return True
    s = str(s).strip()
    if len(s) < 3: return True
    if re.fullmatch(r"[.\s…]+", s): return True
    return False


# -----------------------------
# Data augmentation (context concat + gap masking)
#  - Context concat: 將相鄰樣本拼接成更長上下文，X/Y 同步拼接
#  - Gap masking: 在 X 端隨機遮罩一段 token，替換成 'gap' 或 'big_gap'
# -----------------------------
def _rand_choice(rng: np.random.Generator, n: int, k: int):
    if k <= 0: 
        return np.array([], dtype=int)
    k = min(k, n)
    return rng.choice(n, size=k, replace=False)

def context_concat_augment(
    df: pd.DataFrame,
    ratio: float,
    seed: int,
    sep_token: str = "CTX",
    max_extra: Optional[int] = None,
) -> pd.DataFrame:
    """回傳「新增」的拼接樣本，不會改動原 df。

    拼接方式：
      X_new = clean_x(X_i + ' <CTX> ' + X_{i+1})
      Y_new = clean_y(Y_i + ' <CTX> ' + Y_{i+1})
    注意：clean_x/clean_y 會把 <CTX> 變成 CTX（strip angle），因此 tokenizer 看到的是一般字串 token。
    """
    if ratio <= 0 or len(df) < 2:
        return df.iloc[0:0].copy()

    rng = np.random.default_rng(seed)
    n_pairs = len(df) - 1
    k = int(round(n_pairs * ratio))
    if max_extra is not None:
        k = min(k, max_extra)

    pick = _rand_choice(rng, n_pairs, k)
    extras = []
    tag = f"<{sep_token}>"
    for i in pick:
        x1, y1 = df.iloc[i]["X_clean"], df.iloc[i]["Y_clean"]
        x2, y2 = df.iloc[i+1]["X_clean"], df.iloc[i+1]["Y_clean"]
        x_new = clean_x(f"{x1} {tag} {x2}")
        y_new = clean_y(f"{y1} {tag} {y2}")
        if (not is_bad_text(x_new)) and (not is_bad_text(y_new)):
            row = df.iloc[i].copy()
            row["X_clean"] = x_new
            row["Y_clean"] = y_new
            row["aug"] = "context_concat"
            extras.append(row)

    if len(extras) == 0:
        return df.iloc[0:0].copy()
    return pd.DataFrame(extras).reset_index(drop=True)

def gap_mask_text(
    text: str,
    rng: np.random.Generator,
    span_min: int = 1,
    span_max: int = 3,
) -> str:
    """對單句做一次 span masking（以空白 token 為單位）。"""
    toks = str(text).split()
    if len(toks) < 3:
        return text

    # 避免把 placeholder token 再遮罩
    candidates = [i for i,t in enumerate(toks) if t not in ("gap", "big_gap")]
    if not candidates:
        return text

    span_min = max(1, int(span_min))
    span_max = max(span_min, int(span_max))
    i0 = int(rng.choice(candidates))
    span = int(rng.integers(span_min, span_max + 1))
    j1 = min(len(toks), i0 + span)

    repl = "gap" if (j1 - i0) <= 1 else "big_gap"
    toks[i0:j1] = [repl]
    return " ".join(toks)

def gap_mask_augment(
    df: pd.DataFrame,
    ratio: float,
    seed: int,
    span_min: int = 1,
    span_max: int = 3,
) -> pd.DataFrame:
    """回傳「改寫 X_clean」後的新 df（同尺寸），只對部分樣本套用 gap masking。"""
    if ratio <= 0 or len(df) == 0:
        out = df.copy()
        out["aug"] = out.get("aug", "none")
        return out

    rng = np.random.default_rng(seed)
    out = df.copy()
    n = len(out)
    k = int(round(n * ratio))
    pick = _rand_choice(rng, n, k)
    out["aug"] = out.get("aug", "none")
    for i in pick:
        x = out.at[i, "X_clean"]
        out.at[i, "X_clean"] = gap_mask_text(x, rng, span_min=span_min, span_max=span_max)
        out.at[i, "aug"] = "gap_mask"
    return out

def prepare_train_df(
    base_df: pd.DataFrame,
    seed: int,
    do_context_concat: bool,
    ctx_ratio: float,
    do_gap_mask: bool,
    gap_ratio: float,
    gap_span_min: int,
    gap_span_max: int,
) -> pd.DataFrame:
    """把兩種 augmentation 組合起來：
    1) 先對 base_df 產生 context concat 的「額外樣本」並 append
    2) 再在整體資料上做 gap mask（只改 X）
    """
    df = base_df.copy().reset_index(drop=True)
    df["aug"] = "none"

    if do_context_concat and ctx_ratio > 0:
        extra = context_concat_augment(df, ratio=ctx_ratio, seed=seed)
        if len(extra) > 0:
            df = pd.concat([df, extra], ignore_index=True)

    if do_gap_mask and gap_ratio > 0:
        df = gap_mask_augment(df, ratio=gap_ratio, seed=seed+1, span_min=gap_span_min, span_max=gap_span_max)

    df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)
    return df
# -----------------------------
# Metric (Kaggle sqrt(BLEU*chrF++))
# -----------------------------
def kaggle_score(preds: List[str], refs: List[str]) -> Dict[str,float]:
    bleu = sacrebleu.corpus_bleu(preds, [refs]).score / 100.0
    chrf = sacrebleu.corpus_chrf(preds, [refs], word_order=2).score / 100.0
    s = math.sqrt(max(bleu, 1e-12) * max(chrf, 1e-12))
    return {"bleu": bleu, "chrfpp": chrf, "kaggle_score": s}

def byt5_safe_decode(ids, tok):
    # ids: (seq_len,) list/int array
    ids = [int(t) for t in ids if int(t) != -100]
    ids = [t for t in ids if t not in tok.all_special_ids]
    tokens = tok.convert_ids_to_tokens(ids)
    text = tok.convert_tokens_to_string(tokens)
    return re.sub(r"\s+", " ", text).strip()

def decode_batch(arr, tok):
    return [byt5_safe_decode(x, tok) for x in arr]

# -----------------------------
# Split
# -----------------------------
def stratified_dev_split(df: pd.DataFrame, dev_ratio: float, seed: int):
    rng = np.random.default_rng(seed)
    L = df["X_clean"].str.len().values
    q1, q2 = np.quantile(L, [0.33, 0.66])
    b = np.where(L<=q1,0,np.where(L<=q2,1,2))
    tmp = df.copy()
    tmp["_b"] = b
    dev_idx = []
    for k in [0,1,2]:
        idx = tmp.index[tmp["_b"]==k].tolist()
        rng.shuffle(idx)
        take = int(round(len(idx)*dev_ratio))
        dev_idx += idx[:take]
    dev = tmp.loc[dev_idx].drop(columns=["_b"]).reset_index(drop=True)
    trn = tmp.drop(index=dev_idx).drop(columns=["_b"]).reset_index(drop=True)
    return trn, dev

# -----------------------------
# Seq2Seq Trainer
# -----------------------------
class EncDataset(Dataset):
    def __init__(self, enc):
        self.enc = enc
    def __len__(self): return len(self.enc["input_ids"])
    def __getitem__(self, i):
        return {k: torch.tensor(v[i]) for k,v in self.enc.items()}

def make_enc(tokenizer, X: List[str], Y: Optional[List[str]], max_src: int, max_tgt: int):
    enc = tokenizer(X, max_length=max_src, truncation=True, padding=True)
    if Y is not None:
        lab = tokenizer(text_target=Y, max_length=max_tgt, truncation=True, padding=True)
        enc["labels"] = lab["input_ids"]
    return enc

def train_byt5(
    model_name_or_dir: str,
    out_dir: str,
    train_df: pd.DataFrame,
    dev_df: pd.DataFrame,
    seed: int,
    max_src: int,
    max_tgt: int,
    lr: float,
    epochs: int,
    bs: int,
    grad_accum: int,
    num_beams_eval: int,
    fp16: bool,
    bf16:bool,
    do_eval_during_train: bool,
):
    ensure_dir(out_dir)
    tok = AutoTokenizer.from_pretrained(model_name_or_dir)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_dir)

    if tok.pad_token_id is None:
        tok.pad_token_id = tok.eos_token_id
    model.config.pad_token_id = tok.pad_token_id

    tr_enc = make_enc(tok, train_df["X_clean"].tolist(), train_df["Y_clean"].tolist(), max_src, max_tgt)
    dv_enc = make_enc(tok, dev_df["X_clean"].tolist(), dev_df["Y_clean"].tolist(), max_src, max_tgt)
    tr_ds = EncDataset(tr_enc)
    dv_ds = EncDataset(dv_enc)

    collator = DataCollatorForSeq2Seq(tokenizer=tok, model=model)

    args = Seq2SeqTrainingArguments(
        output_dir=out_dir,
        learning_rate=lr,
        per_device_train_batch_size=bs,
        per_device_eval_batch_size=bs,
        gradient_accumulation_steps=grad_accum,
        num_train_epochs=epochs,
        logging_steps=50,
        report_to="none",
        seed=seed,
        bf16=bf16 and torch.cuda.is_available(),
        fp16=fp16 and torch.cuda.is_available() and (not bf16),

        # 你之前卡住的主因：每個epoch eval+generate 很慢
        # 這裡做成開關
        eval_strategy="epoch" if do_eval_during_train else "no",
        save_strategy="epoch" if do_eval_during_train else "no",
        predict_with_generate=do_eval_during_train,
        generation_num_beams=num_beams_eval,
        generation_max_length=max_tgt,
        load_best_model_at_end=do_eval_during_train,
        metric_for_best_model="eval_kaggle_score",
        greater_is_better=True,
        save_total_limit=2,
    )

    def compute_metrics(eval_pred):
        preds_ids, labels_ids = eval_pred
        labels_ids = np.where(labels_ids == -100, tok.pad_token_id, labels_ids)
        preds = decode_batch(preds_ids, tok)
        refs  = decode_batch(labels_ids, tok)
        s = kaggle_score(preds, refs)
        return {"eval_bleu": s["bleu"], "eval_chrfpp": s["chrfpp"], "eval_kaggle_score": s["kaggle_score"]}

    trainer = Seq2SeqTrainer(
        model=model,
        args=args,
        train_dataset=tr_ds,
        eval_dataset=dv_ds,
        data_collator=collator,
        compute_metrics=compute_metrics if do_eval_during_train else None,
    )
    trainer.train()
    trainer.save_model(out_dir)
    tok.save_pretrained(out_dir)
    return out_dir

@torch.no_grad()
def generate_with_logprob(
    model_dir: str,
    X: List[str],
    max_src: int,
    max_new_tokens: int,
    batch_size: int,
    num_beams: int,
    fp16: bool,
    bf16: bool = False,
):
    """
    產生 pseudo-label 同時回傳一個粗略置信度：avg token logprob
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    tok = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device)
    model.eval()

    if tok.pad_token_id is None:
        tok.pad_token_id = tok.eos_token_id
    model.config.pad_token_id = tok.pad_token_id

    preds, confs = [], []

    for i in range(0, len(X), batch_size):
        batch = X[i:i+batch_size]
        enc = tok(batch, return_tensors="pt", padding=True, truncation=True, max_length=max_src).to(device)

        gen = model.generate(
            **enc,
            num_beams=num_beams,
            max_new_tokens=max_new_tokens,
            return_dict_in_generate=True,
            output_scores=True,
        )
        seqs = gen.sequences  # (B, T)
        texts = decode_batch(seqs.detach().cpu().numpy(), tok)

        # 估 logprob：用 scores（每步的logits after softmax）
        # scores: list length = generated_steps, each (B, vocab)
        # 取每步選到 token 的 logprob 平均（忽略第一個 decoder token）
        step_scores = gen.scores
        # 取生成 token ids（去掉輸入部分，generate 對 encoder-decoder 通常 sequences 只含 decoder）
        # 這裡簡化：把每一步的 argmax/beam token 取出
        # 對 beam search 的 exact mapping 比較複雜；但作為粗置信度，直接用每步 max logprob 也可
        # 這裡用「每步 max logprob」的平均，保守當置信度 proxy
        lp = []
        for sc in step_scores:
            # sc: (B, vocab) logits
            logp = torch.log_softmax(sc, dim=-1)  # (B, vocab)
            lp.append(logp.max(dim=-1).values.detach().cpu().numpy())  # (B,)
        if len(lp) == 0:
            avg_lp = np.zeros(len(texts), dtype=float)
        else:
            avg_lp = np.mean(np.stack(lp, axis=0), axis=0)  # (B,)

        preds.extend(texts)
        confs.extend(avg_lp.tolist())

    return preds, confs

def eval_dev(model_dir: str, dev_df: pd.DataFrame, max_src: int, max_tgt: int, bs: int, num_beams: int):
    tok = AutoTokenizer.from_pretrained(model_dir)
    preds, _ = generate_with_logprob(
        model_dir=model_dir,
        X=dev_df["X_clean"].tolist(),
        max_src=max_src,
        max_new_tokens=max_tgt,
        batch_size=bs,
        num_beams=num_beams,
        fp16=False,
        bf16=True,
    )
    refs = dev_df["Y_clean"].tolist()
    return kaggle_score(preds, refs)

# -----------------------------
# Pseudo-label mixing
# -----------------------------
def make_mixed_train(
    gold_df: pd.DataFrame,
    pseudo_df: pd.DataFrame,
    pseudo_weight: float,
    seed: int
) -> pd.DataFrame:
    """
    最簡單可控的混合：用「重複抽樣」近似權重
    pseudo_weight=0.3 表示 pseudo 大約是 gold 的 30% 份量
    """
    rng = np.random.default_rng(seed)
    if pseudo_weight <= 0:
        return gold_df.copy().reset_index(drop=True)

    n_gold = len(gold_df)
    n_pseudo = int(round(n_gold * pseudo_weight))
    if n_pseudo <= 0:
        return gold_df.copy().reset_index(drop=True)

    # 從 pseudo_df 抽 n_pseudo 筆（可放回）
    idx = rng.integers(0, len(pseudo_df), size=n_pseudo)
    sampled = pseudo_df.iloc[idx].copy()
    mix = pd.concat([gold_df, sampled], ignore_index=True)
    mix = mix.sample(frac=1.0, random_state=seed).reset_index(drop=True)
    return mix

def filter_pseudo_by_conf(pseudo_df: pd.DataFrame, conf_th: float) -> pd.DataFrame:
    if "conf" not in pseudo_df.columns:
        return pseudo_df
    return pseudo_df[pseudo_df["conf"] >= conf_th].reset_index(drop=True)

# -----------------------------
# Main stages
# -----------------------------
def main():
    ap = argparse.ArgumentParser()

    # paths
    ap.add_argument("--train_csv", required=True)
    ap.add_argument("--flagged_csv", required=True, help="train_flagged.csv (含 suspicious 欄)")
    ap.add_argument("--test_csv", required=True)
    ap.add_argument("--sample_sub_csv", required=True)
    ap.add_argument("--unlabeled_csv", default="", help="8000 unlabeled pool (transliteration only)")

    ap.add_argument("--out_dir", required=True)
    ap.add_argument("--seed", type=int, default=42)

    # model
    ap.add_argument("--base_model", default="google/byt5-small")
    ap.add_argument("--max_src", type=int, default=512)
    ap.add_argument("--max_tgt", type=int, default=256)

    # training params
    ap.add_argument("--epochs", type=int, default=10)
    ap.add_argument("--lr", type=float, default=3e-4)
    ap.add_argument("--bs", type=int, default=8)          # A100 可拉高
    ap.add_argument("--grad_accum", type=int, default=2)
    ap.add_argument("--dev_ratio", type=float, default=0.1)
    ap.add_argument("--fp16", action="store_true")
    ap.add_argument("--bf16", action="store_true")

    # generation params
    ap.add_argument("--gen_bs", type=int, default=32)     # A100 可拉高
    ap.add_argument("--gen_beams", type=int, default=1)   # pseudo 建議 1（快且穩）
    ap.add_argument("--eval_beams", type=int, default=2)  # dev eval 可 2

    # pseudo control
    ap.add_argument("--conf_th", type=float, default=-2, help="pseudo confidence threshold (avg max-logprob)")
    ap.add_argument("--pseudo_weight", type=float, default=0.4, help="pseudo amount vs gold in mixing")

    # augmentation switches (結合 best.ipynb 的做法)
    ap.add_argument("--do_context_concat", action="store_true", help="在訓練資料加入相鄰樣本上下文拼接")
    ap.add_argument("--ctx_ratio", type=float, default=0.25, help="context concat 生成量：相鄰pair的比例 (0~1)")
    ap.add_argument("--do_gap_mask", action="store_true", help="在 X 端做 gap/big_gap masking")
    ap.add_argument("--gap_ratio", type=float, default=0.30, help="gap masking 套用到訓練樣本的比例 (0~1)")
    ap.add_argument("--gap_span_min", type=int, default=1)
    ap.add_argument("--gap_span_max", type=int, default=3)

    # export (方便拿去 best.ipynb 的 SentencePiece/Transformer 那套再訓練)
    ap.add_argument("--export_pseudo_for_other_model", action="store_true", help="額外輸出 pseudo CSV（只含 X_clean,Y_clean）")

    # unlabeled control (8000 pool)
    ap.add_argument("--u_part", choices=["A","B"], default="A")
    ap.add_argument("--u_take", type=int, default=2000, help="how many from part to label")
    ap.add_argument("--u_seed", type=int, default=123)

    # stage switches
    ap.add_argument("--do_repair", action="store_true")
    ap.add_argument("--do_selfdistill", action="store_true")
    ap.add_argument("--do_label_unlabeled_byt5", action="store_true")
    ap.add_argument("--do_final_train", action="store_true")
    ap.add_argument("--do_infer", action="store_true")

    args = ap.parse_args()
    ensure_dir(args.out_dir)
    seed_everything(args.seed)

    # -----------------------
    # Load & clean gold
    # -----------------------
    train_raw = pd.read_csv(args.train_csv)
    flagged = pd.read_csv(args.flagged_csv)

    # ===== (A) 先把 train_raw 清成 X_clean / Y_clean（支援多種欄位命名） =====
    # train_raw 可能是：
    #  - transliteration / translation（你最新合併後檔案）
    #  - src_clean / tgt_clean（早期清理版）
    #  - 已經有 X_clean / Y_clean
    if "X_clean" not in train_raw.columns:
        if "transliteration" in train_raw.columns:
            train_raw["X_clean"] = train_raw["transliteration"].apply(clean_x)
        elif "src_clean" in train_raw.columns:
            train_raw["X_clean"] = train_raw["src_clean"].apply(clean_x)
        else:
            raise ValueError("train_csv 缺少可用的 source 欄位（需要 transliteration 或 src_clean 或 X_clean）")

    if "Y_clean" not in train_raw.columns:
        if "translation" in train_raw.columns:
            train_raw["Y_clean"] = train_raw["translation"].apply(clean_y)
        elif "tgt_clean" in train_raw.columns:
            train_raw["Y_clean"] = train_raw["tgt_clean"].apply(clean_y)
        else:
            raise ValueError("train_csv 缺少可用的 target 欄位（需要 translation 或 tgt_clean 或 Y_clean）")

    # 保留非空
    train_raw = train_raw[(train_raw["X_clean"].astype(str).str.len() > 0) & (train_raw["Y_clean"].astype(str).str.len() > 0)].reset_index(drop=True)

    # ===== (B) flagged 也做一致清理（用來做乾淨 dev split + suspicious repair） =====
    flagged["X_clean"] = flagged["transliteration"].apply(clean_x)
    flagged["Y_clean"] = flagged["translation"].apply(clean_y)

    # 保留非空
    flagged = flagged[(flagged["X_clean"].str.len() > 0) & (flagged["Y_clean"].str.len() > 0)].reset_index(drop=True)

    # 固定 dev split：只用 flagged 的「非 suspicious」當作乾淨 gold pool
    gold_filtered = flagged[~flagged["suspicious"]].reset_index(drop=True)
    _, dev_gold = stratified_dev_split(gold_filtered, args.dev_ratio, args.seed)

    # ===== (C) 目標 A：讓 teacher0 更強 =====
    # teacher0 的 train_gold 改用你合併後的 train_csv（train_raw）
    # 同時把 dev 的 oare_id 從 train_raw 排掉，避免資料洩漏
    if "oare_id" in train_raw.columns and "oare_id" in dev_gold.columns:
        dev_ids = set(dev_gold["oare_id"].astype(str))
        train_gold = train_raw[~train_raw["oare_id"].astype(str).isin(dev_ids)].reset_index(drop=True)
    else:
        train_gold = train_raw.copy()

    save_meta = {
        "n_flagged_all": int(len(flagged)),
        "n_flagged_filtered": int(len(gold_filtered)),
        "n_train_raw": int(len(train_raw)),
        "n_train_gold_used_for_teacher0": int(len(train_gold)),
        "n_dev_gold": int(len(dev_gold)),
    }
    with open(os.path.join(args.out_dir, "meta.json"), "w", encoding="utf-8") as f:
        json.dump(save_meta, f, ensure_ascii=False, indent=2)
    # -----------------------
    # Stage 1: train base teacher
    # -----------------------
    teacher0_dir = os.path.join(args.out_dir, "teacher0_byt5")
    if not os.path.exists(os.path.join(teacher0_dir, "config.json")):
        train_byt5(
            model_name_or_dir=args.base_model,
            out_dir=teacher0_dir,
            train_df=prepare_train_df(train_gold, args.seed, args.do_context_concat, args.ctx_ratio, args.do_gap_mask, args.gap_ratio, args.gap_span_min, args.gap_span_max),
            dev_df=dev_gold,
            seed=args.seed,
            max_src=args.max_src,
            max_tgt=args.max_tgt,
            lr=args.lr,
            epochs=args.epochs,
            bs=args.bs,
            grad_accum=args.grad_accum,
            num_beams_eval=args.eval_beams,
            fp16=args.fp16,
            bf16=args.bf16,
            do_eval_during_train=False,  # 關掉訓練中 eval，避免你之前那種每epoch卡很久
        )

    base_scores = eval_dev(teacher0_dir, dev_gold, args.max_src, args.max_tgt, bs=args.gen_bs, num_beams=args.eval_beams)
    print("[Teacher0] DEV:", base_scores)
    with open(os.path.join(args.out_dir, "teacher0_dev.json"), "w", encoding="utf-8") as f:
        json.dump(base_scores, f, ensure_ascii=False, indent=2)

    # -----------------------
    # Stage 2: Repair suspicious (用 teacher0 重標 suspicious)
    # -----------------------
    repaired_csv = os.path.join(args.out_dir, "gold_repaired.csv")
    if args.do_repair:
        sus = flagged[flagged["suspicious"]].reset_index(drop=True)
        keep = flagged[~flagged["suspicious"]].reset_index(drop=True)

        if len(sus) > 0:
            preds, confs = generate_with_logprob(
                model_dir=teacher0_dir,
                X=sus["X_clean"].tolist(),
                max_src=args.max_src,
                max_new_tokens=args.max_tgt,
                batch_size=args.gen_bs,
                num_beams=args.gen_beams,
                fp16=args.fp16,
                bf16=args.bf16,
            )
            sus2 = sus.copy()
            sus2["Y_pseudo"] = preds
            sus2["conf"] = confs
            sus2 = sus2[~sus2["Y_pseudo"].apply(is_bad_text)].reset_index(drop=True)
            sus2 = filter_pseudo_by_conf(sus2, args.conf_th)

            # 用 pseudo 修補 suspicious 的 target
            sus2["Y_clean"] = sus2["Y_pseudo"]
            sus2 = sus2.drop(columns=["Y_pseudo"])

            repaired = pd.concat([keep, sus2], ignore_index=True)
        else:
            repaired = flagged.copy()

        repaired = repaired.sample(frac=1.0, random_state=args.seed).reset_index(drop=True)
        repaired.to_csv(repaired_csv, index=False)
        print("[Repair] saved:", repaired_csv)
    else:
        if os.path.exists(repaired_csv):
            repaired = pd.read_csv(repaired_csv)
        else:
            repaired = flagged.copy()

    # 重新產生「修補後的 filtered gold」：疑似錯配已被修補，所以可以全納入 gold
    repaired["X_clean"] = repaired["transliteration"].apply(clean_x)
    repaired["Y_clean"] = repaired["translation"].apply(clean_y)
    repaired = repaired[(repaired["X_clean"].str.len() > 0) & (repaired["Y_clean"].str.len() > 0)].reset_index(drop=True)

    train_gold2, dev_gold2 = stratified_dev_split(repaired, args.dev_ratio, args.seed)

    # -----------------------
    # Stage 3: Self-distillation on repaired gold (對 1468/或整個 repaired 產 pseudo 再混合)
    # -----------------------
    student1_dir = os.path.join(args.out_dir, "student1_selfdistill")
    pseudo_all_csv = os.path.join(args.out_dir, "pseudo_all_repaired.csv")

    if args.do_selfdistill:
        # 先對「repaired gold 的 train split」產 pseudo，避免把 dev 泄漏進訓練
        preds, confs = generate_with_logprob(
            model_dir=teacher0_dir,
            X=train_gold2["X_clean"].tolist(),
            max_src=args.max_src,
            max_new_tokens=args.max_tgt,
            batch_size=args.gen_bs,
            num_beams=args.gen_beams,
            fp16=args.fp16,
            bf16=args.bf16,
        )
        pseudo_df = train_gold2.copy()
        pseudo_df["Y_clean"] = preds
        pseudo_df["conf"] = confs
        pseudo_df = pseudo_df[~pseudo_df["Y_clean"].apply(is_bad_text)].reset_index(drop=True)
        pseudo_df = filter_pseudo_by_conf(pseudo_df, args.conf_th)
        pseudo_df.to_csv(pseudo_all_csv, index=False)
        print("[SelfDistill] pseudo saved:", pseudo_all_csv, "n=", len(pseudo_df))

        if args.export_pseudo_for_other_model:
            pseudo_min = pseudo_df[["X_clean","Y_clean"]].copy()
            pseudo_min.to_csv(os.path.join(args.out_dir, "pseudo_selfdistill_min.csv"), index=False)

        # 混合：gold (repaired) + pseudo(低權重)
        mixed = make_mixed_train(train_gold2, pseudo_df, pseudo_weight=args.pseudo_weight, seed=args.seed)
        mixed.to_csv(os.path.join(args.out_dir, "mixed_train_student1.csv"), index=False)

        train_byt5(
            model_name_or_dir=teacher0_dir,   # 用 teacher0 當初始化更穩
            out_dir=student1_dir,
            train_df=prepare_train_df(mixed, args.seed, args.do_context_concat, args.ctx_ratio, args.do_gap_mask, args.gap_ratio, args.gap_span_min, args.gap_span_max),
            dev_df=dev_gold2,
            seed=args.seed,
            max_src=args.max_src,
            max_tgt=args.max_tgt,
            lr=args.lr * 0.6,                 # self-distill 建議稍降 lr
            epochs=max(3, args.epochs//2),     # 不用跑滿10，通常 3~5 足夠
            bs=args.bs,
            grad_accum=args.grad_accum,
            num_beams_eval=args.eval_beams,
            fp16=args.fp16,
            bf16=args.bf16,
            do_eval_during_train=False,
        )

        s1 = eval_dev(student1_dir, dev_gold2, args.max_src, args.max_tgt, bs=args.gen_bs, num_beams=args.eval_beams)
        print("[Student1] DEV:", s1)
        with open(os.path.join(args.out_dir, "student1_dev.json"), "w", encoding="utf-8") as f:
            json.dump(s1, f, ensure_ascii=False, indent=2)
    else:
        if not os.path.exists(os.path.join(student1_dir, "config.json")):
            student1_dir = teacher0_dir

    # -----------------------
    # Stage 4: ByT5 label unlabeled pool (8000 的一部分)
    # -----------------------
    unlabeled_pseudo_csv = os.path.join(args.out_dir, f"unlabeled_{args.u_part}_byt5_pseudo.csv")
    if args.do_label_unlabeled_byt5:
        if not args.unlabeled_csv:
            raise ValueError("--unlabeled_csv required for do_label_unlabeled_byt5")

        U = pd.read_csv(args.unlabeled_csv)
        # 期待至少有 transliteration 欄
        if "transliteration" not in U.columns:
            raise ValueError("unlabeled_csv must have column: transliteration")

        U["X_clean"] = U["transliteration"].apply(clean_x)
        U = U[U["X_clean"].str.len() > 0].reset_index(drop=True)

        # split A/B
        rng = np.random.default_rng(args.u_seed)
        idx = np.arange(len(U))
        rng.shuffle(idx)
        mid = len(U)//2
        if args.u_part == "A":
            pick = idx[:mid]
        else:
            pick = idx[mid:]
        pick = pick[:min(args.u_take, len(pick))]
        U_part = U.iloc[pick].copy().reset_index(drop=True)

        # 用 student1（若有）產 pseudo（比 teacher0 更強通常更好）
        teacher_for_u = student1_dir if os.path.exists(os.path.join(student1_dir, "config.json")) else teacher0_dir
        preds, confs = generate_with_logprob(
            model_dir=teacher_for_u,
            X=U_part["X_clean"].tolist(),
            max_src=args.max_src,
            max_new_tokens=args.max_tgt,
            batch_size=args.gen_bs,
            num_beams=args.gen_beams,
            fp16=args.fp16,
            bf16=args.bf16,
        )
        U_part["Y_clean"] = preds
        U_part["conf"] = confs
        U_part = U_part[~U_part["Y_clean"].apply(is_bad_text)].reset_index(drop=True)
        U_part = filter_pseudo_by_conf(U_part, args.conf_th)

        U_part.to_csv(unlabeled_pseudo_csv, index=False)
        print("[Unlabeled ByT5 pseudo] saved:", unlabeled_pseudo_csv, "n=", len(U_part))

        if args.export_pseudo_for_other_model:
            u_min = U_part[["X_clean","Y_clean"]].copy()
            u_min.to_csv(os.path.join(args.out_dir, f"unlabeled_{args.u_part}_pseudo_min.csv"), index=False)

    # -----------------------
    # Stage 5: Final train ByT5 on gold + (unlabeled pseudo) + (optionally) more pseudo
    # -----------------------
    final_dir = os.path.join(args.out_dir, "byt5_final")
    if args.do_final_train:
        # gold：以 repaired 為主，再把 args.train_csv（含你合併的 train）併入，讓 final stage1 的 gold 更完整
        gold_all = repaired.copy()
        if "X_clean" not in gold_all.columns:
            if "transliteration" in gold_all.columns:
                gold_all["X_clean"] = gold_all["transliteration"].apply(clean_x)
            elif "src_clean" in gold_all.columns:
                gold_all["X_clean"] = gold_all["src_clean"].apply(clean_x)
            else:
                raise ValueError("repaired gold must have transliteration (or src_clean) column to build X_clean")
        if "Y_clean" not in gold_all.columns:
            if "translation" in gold_all.columns:
                gold_all["Y_clean"] = gold_all["translation"].apply(clean_y)
            elif "tgt_clean" in gold_all.columns:
                gold_all["Y_clean"] = gold_all["tgt_clean"].apply(clean_y)
            else:
                raise ValueError("repaired gold must have translation (or tgt_clean) column to build Y_clean")
        gold_all = gold_all[(gold_all["X_clean"].str.len()>0) & (gold_all["Y_clean"].str.len()>0)].reset_index(drop=True)

        # stage2 想維持「純 repaired gold」的話，保留一份不含 train_extra 的版本
        gold_stage2 = gold_all.copy()

        # 併入 train_csv（可能是你合併後的 3k+ 筆資料）
        train_extra = train_raw.copy()

        # 兼容欄位命名：transliteration/translation 或 src_clean/tgt_clean
        if "X_clean" not in train_extra.columns:
            if "transliteration" in train_extra.columns:
                train_extra["X_clean"] = train_extra["transliteration"].apply(clean_x)
            elif "src_clean" in train_extra.columns:
                train_extra["X_clean"] = train_extra["src_clean"].apply(clean_x)
            else:
                raise ValueError("train_csv must have transliteration (or src_clean) column to build X_clean")
        if "Y_clean" not in train_extra.columns:
            if "translation" in train_extra.columns:
                train_extra["Y_clean"] = train_extra["translation"].apply(clean_y)
            elif "tgt_clean" in train_extra.columns:
                train_extra["Y_clean"] = train_extra["tgt_clean"].apply(clean_y)
            else:
                raise ValueError("train_csv must have translation (or tgt_clean) column to build Y_clean")

        train_extra = train_extra[(train_extra["X_clean"].str.len()>0) & (train_extra["Y_clean"].str.len()>0)].reset_index(drop=True)

        # 去重：優先保留 repaired（gold_all），避免同一 oare_id 重複
        if "oare_id" in gold_all.columns and "oare_id" in train_extra.columns:
            exist = set(gold_all["oare_id"].astype(str))
            train_extra = train_extra[~train_extra["oare_id"].astype(str).isin(exist)].reset_index(drop=True)
        else:
            train_extra = train_extra.drop_duplicates(subset=["X_clean","Y_clean"]).reset_index(drop=True)

        if len(train_extra) > 0:
            gold_all = pd.concat([gold_all, train_extra], ignore_index=True)
            gold_all = gold_all.sample(frac=1.0, random_state=args.seed).reset_index(drop=True)

        # unlabeled pseudo
        # stage1 的 gold（含你合併的 train_csv）
        gold_stage1 = gold_all

        # unlabeled pseudo（如果有）
        pseudo_list = []
        if os.path.exists(unlabeled_pseudo_csv):
            pseudo_list.append(pd.read_csv(unlabeled_pseudo_csv))
        # 你如果之後還有 LLM pseudo，也可以再 append 進來（同欄位 X_clean, Y_clean 即可）
        # pseudo_list.append(pd.read_csv("unlabeled_B_llm_pseudo.csv"))
        pseudo_list.append(pd.read_csv("/content/llm_out/unlabeled_B_llm_pseudo.csv"))

        if len(pseudo_list) > 0:
            pseudo_u = pd.concat(pseudo_list, ignore_index=True)
            pseudo_u = pseudo_u[["X_clean","Y_clean"]].dropna().reset_index(drop=True)
            mixed_final = make_mixed_train(gold_stage1, pseudo_u, pseudo_weight=args.pseudo_weight, seed=args.seed)
        else:
            mixed_final = gold_stage1

        train_final, dev_final = stratified_dev_split(mixed_final, args.dev_ratio, args.seed)

        # curriculum：先混合訓練，再用 gold 微調一小段（可選，這裡直接做兩段）
        stage1_dir = os.path.join(args.out_dir, "byt5_final_stage1")
        train_byt5(
            model_name_or_dir=student1_dir,    # 用 student1 初始化
            out_dir=stage1_dir,
            train_df=prepare_train_df(train_final, args.seed, args.do_context_concat, args.ctx_ratio, args.do_gap_mask, args.gap_ratio, args.gap_span_min, args.gap_span_max),
            dev_df=dev_final,
            seed=args.seed,
            max_src=args.max_src,
            max_tgt=args.max_tgt,
            lr=args.lr,
            epochs=max(3, args.epochs//2),
            bs=args.bs,
            grad_accum=args.grad_accum,
            num_beams_eval=args.eval_beams,
            fp16=args.fp16,
            bf16=args.bf16,
            do_eval_during_train=False,
        )

        # stage2：只用 gold_all 微調（把模型拉回真標籤分佈）
        train_gold3, dev_gold3 = stratified_dev_split(gold_stage2, args.dev_ratio, args.seed)
        train_byt5(
            model_name_or_dir=stage1_dir,
            out_dir=final_dir,
            train_df=train_gold3,
            dev_df=dev_gold3,
            seed=args.seed,
            max_src=args.max_src,
            max_tgt=args.max_tgt,
            lr=args.lr * 0.5,
            epochs=2,
            bs=args.bs,
            grad_accum=args.grad_accum,
            num_beams_eval=args.eval_beams,
            fp16=args.fp16,
            bf16=args.bf16,
            do_eval_during_train=False,
        )

        fscore = eval_dev(final_dir, dev_gold3, args.max_src, args.max_tgt, bs=args.gen_bs, num_beams=args.eval_beams)
        print("[Final ByT5] DEV:", fscore)
        with open(os.path.join(args.out_dir, "byt5_final_dev.json"), "w", encoding="utf-8") as f:
            json.dump(fscore, f, ensure_ascii=False, indent=2)

    # -----------------------
    # Stage 6: Inference to submission
    # -----------------------
    if args.do_infer:
        model_for_test = final_dir if os.path.exists(os.path.join(final_dir, "config.json")) else (student1_dir if os.path.exists(os.path.join(student1_dir,"config.json")) else teacher0_dir)

        test = pd.read_csv(args.test_csv)
        sub = pd.read_csv(args.sample_sub_csv)

        if "transliteration" not in test.columns:
            raise ValueError("test.csv must have transliteration column")

        test["X_clean"] = test["transliteration"].apply(clean_x)
        preds, _ = generate_with_logprob(
            model_dir=model_for_test,
            X=test["X_clean"].tolist(),
            max_src=args.max_src,
            max_new_tokens=args.max_tgt,
            batch_size=args.gen_bs,
            num_beams=max(2, args.eval_beams),   # test 可稍微提高 beam
            fp16=args.fp16,
            bf16=args.bf16,
        )

        # 對齊 sample_submission
        if "id" in sub.columns and "id" in test.columns:
            pred_df = pd.DataFrame({"id": test["id"].values, "translation": preds})
            out = sub[["id"]].merge(pred_df, on="id", how="left")
        else:
            out = sub.copy()
            out["translation"] = preds[:len(out)]

        out_path = os.path.join(args.out_dir, "submission.csv")
        out.to_csv(out_path, index=False)
        print("[Infer] saved:", out_path)

if __name__ == "__main__":
    main()